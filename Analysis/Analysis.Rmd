---
title: "Longitudinal gameplay and well-being"
author:
  - name: <a href="https://vuorre.netlify.app">Matti Vuorre</a>
    affiliation: <a href="https://www.oii.ox.ac.uk/people/matti-vuorre/">University of Oxford</a>
  - name: Your Name Here
    affiliation: University
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    theme: yeti
    highlight: kate
    self_contained: true
    number_sections: false
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r packages, include = FALSE}
library(knitr)
library(scales)
library(janitor)
library(ggbeeswarm)
library(gtsummary)
library(bayestestR)
library(brms)
library(tidybayes)
library(broom)
library(here)
library(lubridate)
library(naniar)
library(ggtext)
library(emmeans)
library(ggstance)
library(ggdist)
library(patchwork)
library(readxl)
library(lavaan)
library(GGally)
# remotes::install_github("jflournoy/riclpmr")  
library(riclpmr)
library(ggnewscale)
library(showtext)
library(tidyverse)
```

```{r setup, include=FALSE}
# Knitr options
opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  cache = TRUE,
  error = FALSE,
  message = FALSE,
  fig.align = "center",
  fig.retina = 2
)
# Plotting options
Font <- "Titillium Web"
font_add_google(Font, Font)
theme_set(
  theme_linedraw(
    base_family = Font, 
    base_size = 12) +
    theme(
      panel.grid.minor = element_blank(),
      panel.grid.major.x = element_blank()
    )
)
# MCMC settings
options(mc.cores = parallel::detectCores(logical = FALSE))
if (require("cmdstanr")) options(brms.backend = "cmdstanr")

# Run this to delete fitted models and refit
# unlink(list.files(pattern = "brm-.*\\.rds"))
```

# Preface

This document contains the analyses supporting "A longitudinal study of video game play and well-being" <doi>

# Data cleaning

## Survey

We first cleaned the raw qualtrics data of sensitive info and placed the resulting table in `Data/qualtrics.csv`. We proceed now to clean that file.

- We only include people at each wave if they answered at least one question in that wave

```{r}
d <- read_csv(here("Data", "qualtrics.csv"))

# Rename block & item order variables for easier processing
d <- d %>%
  rename(
    .spane = spane_DO,
    .pens = pens_needs_DO,
    .blocks1 = FL_20_DO,
    .blocks2 = FL_22_DO
  )

# Take out rows (1 row = 1 participant's responses at one wave)
# where no survey questions answered; this defines a "participant"
d$responses <- apply(
  select(d, starts_with("spane_"), starts_with("pens_"), csas), 1,
  function(x) sum(!is.na(x))
)
d <- d %>%
  filter(responses > 0) %>%
  select(-responses)

# Has played?
d <- d %>%
  mutate(played = !str_detect(played, "NOT"))

# Estimated time played
d <- d %>%
  mutate(minutes = minutes / 60) %>%
  mutate(hours_est = rowSums(select(., hours, minutes), na.rm = TRUE)) %>%
  # sum above returns 0 if both hours and minutes are NA, fix here:
  mutate(hours_est = ifelse(is.na(hours) & is.na(minutes), NA, hours_est)) %>%
  select(-minutes, -hours)

# Scale responses
d <- d %>%
  mutate(
    across(
      starts_with("spane_"),
      function(x) factor(x, levels = c("Very rarely or never", "Rarely", "Occasionally", "Sometimes", "Frequently", "Often", "Very often or always"))
    )
  )
d <- d %>%
  mutate(
    across(
      starts_with("pens_"),
      function(x) factor(x, levels = c("Strongly disagree", "Disagree", "Somewhat disagree", "Neither agree nor disagree", "Somewhat agree", "Agree", "Strongly agree"))
    )
  )
d <- d %>%
  mutate(
    across(c(starts_with("spane_"), starts_with("pens_")), as.numeric)
  )

# Reverse scored items
reverse_items <- c("pens_needs_9", "pens_motivations_2", "pens_motivations_3")
d <- d %>%
  mutate(
    across(all_of(reverse_items), ~ 8 - .x)
  )

# Subscale items
spane_pos_items <- paste0("spane_", c(1, 3, 5, 7, 10, 12))
spane_neg_items <- paste0("spane_", c(2, 4, 6, 8, 9, 11))
autonomy_items <- paste0("pens_needs_", 1:3)
competence_items <- paste0("pens_needs_", 4:6)
relatedness_items <- paste0("pens_needs_", 7:9)
enjoyment_items <- paste0("pens_motivations_", 1:4)
extrinsic_items <- paste0("pens_motivations_", 5:8)

# Create scale scores
d <- d %>%
  mutate(
    spane_pos = rowMeans(select(., all_of(spane_pos_items)), na.rm = TRUE),
    spane_neg = rowMeans(select(., all_of(spane_neg_items)), na.rm = TRUE),
    spane = spane_pos - spane_neg,
    enjoyment = rowMeans(select(., all_of(enjoyment_items)), na.rm = TRUE),
    extrinsic = rowMeans(select(., all_of(extrinsic_items)), na.rm = TRUE),
    needs = rowMeans(
      select(., all_of(c(autonomy_items, competence_items))),
      na.rm = TRUE
    ),
    autonomy = rowMeans(select(., all_of(autonomy_items)), na.rm = TRUE),
    competence = rowMeans(select(., all_of(competence_items)), na.rm = TRUE),
    relatedness = rowMeans(select(., all_of(relatedness_items)), na.rm = TRUE),
  )

# Remove items
d <- d %>%
  select(-all_of(c(spane_pos_items, spane_neg_items, autonomy_items, competence_items, relatedness_items, enjoyment_items, extrinsic_items)))


# Make wave a nicely labelled factor
d <- d %>%
  mutate(Wave = factor(wid, levels = 1:3, labels = paste0("Wave ", 1:3)))

# Abbreviate long game names
d <- d %>%
  mutate(
    game = ifelse(game == "Animal Crossing: New Horizons", "AC:NH", game)
  )

# Gender as factor
d <- d %>%
  mutate(gender = factor(gender))

# Calculate interval between waves
survey_intervals <- d %>%
  select(game, pid, wid, StartDate) %>%
  arrange(pid, wid) %>%
  # Make sure that there is a row for each subject X wave 
  # so interval is calculated correctly
  complete(wid, nesting(pid, game)) %>%
  arrange(pid, wid) %>%
  group_by(pid) %>%
  # Interval between waves in days
  mutate(
    interval = (as.numeric(StartDate) - as.numeric(lag(StartDate))) / 60 / 60 / 24
  ) %>%
  ungroup() %>%
  select(wid, pid, game, interval)
d <- left_join(d, survey_intervals)

# Prettier names for tables/figures
d <- d %>%
  rename_with(
    str_to_upper,
    c(starts_with("spane"), csas)
  ) %>%
  rename_with(
    str_to_title,
    c(age, gender, experience, game, company, enjoyment:interval)
  )

# Get data on invite dates and Ns
invites <- read_excel(here("Data", "invites.xlsx"))
```

## Telemetry

### EA

This data is at the player by match level: Each row indicates the start and end time of a match for that particular player.

Note there's a lot of information in this table but we only use the session times.

todo I think this allows for a telemetry observation to count for two surveys if their interval was shorter than 14 days but that is okay because survey asked for prior two weeks, or is it? Maybe or maybe not.

```{r}
telemetry_ea <- read_csv(here("Data", "telemetry_ea.csv"))

# Select relevant variables
telemetry_ea <- telemetry_ea %>%
  select(
    pid, game_start_time_utc, game_end_time_utc
  ) %>%
  transmute(
    pid,
    game_start = as_datetime(mdy_hm(game_start_time_utc), tz = "UTC"),
    game_end = as_datetime(mdy_hm(game_end_time_utc), tz = "UTC")
  )

# Include only matches that happened in the two weeks prior to each survey
telemetry_ea <- telemetry_ea %>%
  left_join(
    d %>%
      filter(Game == "Apex Legends") %>%
      select(pid, wid, StartDate)
  ) %>%
  # Is in window if match started or ended in the fourteen days prior to survey
  filter(
    game_start %within% interval(StartDate - days(14), StartDate) |
      game_end %within% interval(StartDate - days(14), StartDate)
  )


# Calculate durations
telemetry_ea <- telemetry_ea %>%
  mutate(Hours = as.numeric(game_end - game_start) / 60 / 60)

# Summarise per wave
telemetry_ea <- telemetry_ea %>%
  group_by(pid, wid) %>%
  summarise(Hours = sum(Hours)) %>%
  ungroup()

# Join back to survey data
d <- left_join(d, telemetry_ea)

# If telemetry was not obtained, participant didn't play and therefore gets 0
# instead of NA. This is easier to do once all data is in.
d <- d %>%
  mutate(Hours = ifelse(Game == "Apex Legends" & is.na(Hours), 0, Hours))
```

### CCP

### Microsoft

### Square Enix

### Sony

Session end times not logged.

### Ubisoft

# Demographics

Before exclusions

```{r}
theme_gtsummary_compact()
d %>%
  distinct(pid, Game, Age, Experience, Gender) %>%
  select(-pid) %>%
  tbl_summary(by = Game, missing_text = "Missing") %>%
  add_overall() %>% 
  bold_labels() %>% 
  italicize_levels()  
```

# Survey descriptives

## Response rate & retention

We calculate response rates and retention with the above assumptions (e.g. a "participant" is someone who answered at least one survey item).

```{r}
# Create a table where wave 0 are number of invites,
# then calculate response rate / retention at each wave.
# This assumes there are no new participants at wave 3
# (people who didn't participate in wave 2 showed up at wave 3).
bind_rows(select(invites, -date), count(d, Game, wid)) %>%
  arrange(Game, wid) %>%
  group_by(Game) %>%
  mutate(
    R_rate = percent(n / lag(n), .1),
    n = comma(n)
  ) %>%
  pivot_wider(names_from = wid, values_from = c(n, R_rate)) %>%
  mutate(
    Invites = n_0,
    `Wave 1` = str_glue("{n_1} ({R_rate_1})"),
    `Wave 2` = str_glue("{n_2} ({R_rate_2})"),
    `Wave 3` = str_glue("{n_3} ({R_rate_3})")
  ) %>%
  select(Game, Invites:`Wave 3`) %>%
  mutate(across(everything(), ~ str_replace(., "NA", "0"))) %>%
  kable(caption = "Number of people (response/retention rate) participating at each wave.")
```

How many participants completed N waves

```{r}
d %>%
  count(Game, pid, name = "Waves") %>%
  tabyl(Game, Waves) %>%
  as_tibble() %>%
  rowwise() %>%
  mutate(Total = sum(`1`, `2`, `3`)) %>%
  ungroup() %>%
  mutate(across(`1`:`3`, .fns = list(p = function(x) percent(x / Total, .1)))) %>%
  mutate(
    `1 wave` = str_glue("{`1`} ({`1_p`})"),
    `2 waves` = str_glue("{`2`} ({`2_p`})"),
    `3 waves` = str_glue("{`3`} ({`3_p`})")
  ) %>%
  select(Game, Total, `1 wave`:`3 waves`) %>%
  kable(caption = "Number of people (percentage) completing 1/2/3 waves per company.")
d %>%
  count(pid, name = "Waves") %>%
  tabyl(Waves) %>%
  adorn_pct_formatting() %>%
  as_tibble() %>%
  kable(caption = "Number of people (percentage) completing 1/2/3 waves total.")
```

## Response dates

```{r}
d %>%
  mutate(Date = as_date(StartDate)) %>%
  count(Game, Wave, Date) %>%
  ggplot(
    aes(Date, n, fill = Wave)
  ) +
  geom_col() +
  scale_y_continuous(
    "Responses",
    breaks = pretty_breaks(10),
    expand = expansion(c(0, .1)),
  ) +
  scale_x_date(
    "Date",
    date_breaks = "7 day", date_labels = "%b %d", date_minor_breaks = "1 day"
  ) +
  # geom_text(aes(label = n), nudge_y = 10) +
  facet_wrap("Game", scales = "free_y", ncol = 2)
```

Response times in BST

```{r}
d %>%
  mutate(Hour = hour(StartDate)) %>%
  count(Game, Wave, Hour) %>%
  ggplot(aes(Hour, y = n, fill = Wave)) +
  scale_y_continuous(
    "Responses",
    breaks = pretty_breaks(10),
    expand = expansion(c(0, .1)),
  ) +
  scale_x_continuous(
    breaks = seq(0, 21, by = 3),
    expand = expansion(c(0.01)),
    labels = function(x) paste0(x, ":00")
  ) +
  geom_col() +
  facet_wrap("Game", scales = "free", ncol = 2)
```

### Durations between waves

Participants could respond with variable delays due to variation in email schedules and late responding. So we also check the actual intervals between completing waves

```{r}
d %>%
  select(wid, Game, Interval) %>%
  group_by(Game, wid) %>%
  filter(wid > 1) %>%
  summarise(
    min = min(Interval, na.rm = TRUE) %>% round(1),
    q25 = quantile(Interval, .25, na.rm = TRUE) %>% round(1),
    median = median(Interval, na.rm = TRUE) %>% round(1),
    q75 = quantile(Interval, .75, na.rm = TRUE) %>% round(1),
    max = max(Interval, na.rm = TRUE) %>% round(1),
    n = n()
  ) %>%
  kable(caption = "Interval durations preceding waves 2 and 3.")
d %>%
  filter(Wave != "Wave 1") %>%
  mutate(Wave = fct_drop(Wave)) %>%
  ggplot(aes(Interval)) +
  geom_vline(xintercept = 14, size = .2) +
  geom_histogram(binwidth = 1, col = "white") +
  scale_y_continuous(
    "Count",
    expand = expansion(c(0, .1))
  ) +
  scale_x_continuous(
    "Days between responding",
    breaks = seq(0, 28, by = 7)
  ) +
  facet_grid(Game ~ Wave, scales = "free_y")
```

## How many people had telemetry

For at least one wave

```{r}
d %>%
  mutate(has_telemetry = !is.na(Hours)) %>%
  group_by(Game, pid) %>%
  summarise(`Has telemetry` = any(has_telemetry)) %>%
  select(-pid) %>%
  tbl_summary(by = Game)
```

## Exclusions

As in our previous study we exclude observations more extreme than 6SD

```{r}
d <- d %>%
  pivot_longer(c(Hours, hours_est:Extrinsic)) %>%
  group_by(Game, name) %>%
  mutate(z = as.numeric(scale(value)))
```

```{r}
# This is what are taken out
d %>%
  summarise(
    Extremes = sum(abs(z >= 6), na.rm = TRUE),
    Extremes_p = percent(Extremes / n(), accuracy = .01)
  ) %>%
  filter(Extremes > 0)
d %>%
  group_by(name) %>%
  summarise(
    Extremes = sum(abs(z >= 6), na.rm = TRUE),
    Extremes_p = percent(Extremes / n(), accuracy = .01)
  ) %>%
  filter(Extremes > 0)
```

```{r}
d <- d %>%
  mutate(value = ifelse(abs(z >= 6), NA, value)) %>%
  select(-z) %>%
  pivot_wider(names_from = "name", values_from = "value") %>%
  ungroup()
```

# Univariate analyses

## Key variables over time

Note that `played` variable is not useful; it was not collected in waves 2 and 3, and sometimes people did estimate to have time played even though reported not playing :/ 

```{r}
d %>%
  mutate(Hours_over_40 = Hours > 40) %>%
  tabyl(Hours_over_40) %>%
  adorn_pct_formatting()
tmp <- d %>%
  select(
    Game, Wave, pid,
    Hours, CSAS, SPANE,
    Enjoyment, Extrinsic, Needs
  ) %>%
  pivot_longer(Hours:Needs) %>%
  drop_na(value) %>%
  filter(value <= 40) %>%
  mutate(
    name = factor(
      name,
      levels = c("Hours", "SPANE", "CSAS", "Extrinsic", "Enjoyment", "Needs"),
      labels = c("Hours played", "Affect", "Life satisfaction", "Extrinsic motivation", "Intrinsic motivation", "Needs satisfaction")
    )
  )
tmp %>%
  # Order points from low to high value within panel and wave
  # so dodging doesn't look like a mess.
  mutate(within_panel_wave_order = str_glue("{name}.{Game}.{Wave}")) %>%
  group_by(name, Wave, Game) %>%
  mutate(mv = mean(value, na.rm = TRUE)) %>%
  ungroup() %>%
  arrange(name, Wave, mv) %>%
  mutate(within_panel_wave_order = fct_inorder(within_panel_wave_order)) %>%
  ggplot(
    aes(
      Wave, value,
      fill = Game, col = Game, group = within_panel_wave_order
    )
  ) +
  scale_x_discrete(labels = 1:3, expand = expansion(c(0.1, .1))) +
  scale_color_brewer(palette = "Set1", aesthetics = c("fill", "color")) +
  scale_y_continuous(
    breaks = pretty_breaks(),
    expand = expansion(.025)
  ) +
  geom_blank() +
  stat_halfeye(
    alpha = .2, height = .02, normalize = "panels", adjust = 1.1,
    point_interval = NULL, show.legend = FALSE,
    position = position_nudge(x = .05)
  ) +
  stat_summary(
    fun.data = mean_cl_normal, fatten = 1,
    position = position_dodge(.08)
  ) +
  facet_wrap("name", scales = "free_y", ncol = 3) +
  labs(y = "Value") +
  guides(
    color = guide_legend(nrow = 2, byrow = TRUE),
    fill = guide_legend(nrow = 2, byrow = TRUE)
  ) +
  theme(
    legend.position = "bottom", 
    legend.justification = "left", 
    legend.margin = margin(l = -12),
    legend.title = element_blank(), 
    legend.text = element_text(margin = margin(l = -8), size = 8)
  )
ggsave(here("Output/Figures/Figure-1.png"), width = 5, height = 6)
```

# Bivariate relations

```{r}
d %>%
  select(
    CSAS, SPANE, Needs, 
    Relatedness, Enjoyment, Extrinsic, Hours, hours_est
  ) %>%
  psych::cor.plot(
    scale = FALSE, stars = FALSE,
    xlas = 2, show.legend = FALSE,
    main = "Correlations of main variables"
  )
```

# Models

## Simple correlation

A new data frame for e.g. regression models

```{r}
# within and between component of predictors
# !! for now replace missing telemetry with subjective estimate !!
d_model <- d %>%
  ### !! Remove this when data is available !!
  mutate(Hours = coalesce(Hours, hours_est)) %>%
  select(Game, pid, Wave, Hours, SPANE, CSAS, Needs, Enjoyment) %>%
  arrange(Game, pid, Wave) %>%
  # Grand mean centered hours
  mutate(Hours_g = Hours - mean(Hours, na.rm = T)) %>%
  # Subject-mean Hours_g, and deviations therein
  group_by(Game, pid) %>%
  mutate(
    Hours_b = mean(Hours_g, na.rm = T),
    Hours_w = Hours_g - Hours_b
  ) %>%
  ungroup()
```

Regression coefficients (standardized and raw) of well-being predicted from game play need satisfaction, and intrinsic motivation (enjoyment).

```{r}
d_model %>%
  rename(Affect = SPANE, `Life satisfaction` = CSAS) %>%
  mutate(Hours = Hours/10) %>% 
  pivot_longer(
    c(Affect, `Life satisfaction`), 
    names_to = "Outcome", values_to = "Outcome_value"
  ) %>%
  pivot_longer(
    c(Needs, Enjoyment, Hours), 
    names_to = "Predictor", values_to = "Predictor_value"
  ) %>% 
  group_by(Game, Outcome, Predictor, Wave) %>%
  summarise(
    std = list(tidy(lm(scale(Outcome_value) ~ scale(Predictor_value), data = cur_data()), conf.int = TRUE)),
    raw = list(tidy(lm(Outcome_value ~ Predictor_value, data = cur_data()), conf.int = TRUE)),
  ) %>%
  pivot_longer(std:raw, names_to = "Model") %>%
  unnest(value) %>%
  filter(term != "(Intercept)") %>%
  mutate(Panel = str_glue("{Outcome} ({Model})")) %>%
  mutate(
    Panel = factor(
      Panel,
      levels = c(
        "Affect (raw)", "Life satisfaction (raw)",
        "Affect (std)", "Life satisfaction (std)"
      )
    )
  ) %>%
  ggplot(aes(estimate, Game, col = Wave)) +
  geom_vline(xintercept = 0, lty = 2, size = .25) +
  scale_x_continuous(
    "Bivariate regression coefficient (95%CI)",
    breaks = pretty_breaks()
  ) +
  geom_pointrangeh(
    aes(xmin = conf.low, xmax = conf.high),
    size = .3,
    position = position_dodge2v(.4)
  ) +
  facet_grid(Panel~Predictor, scales = "free_x") +
  theme(
    axis.title.x = element_markdown(),
    axis.title.y = element_blank(),
    legend.position = "bottom",
    legend.title = element_blank()
  )
```

# RICLPM

Wrangle data to a format where lavaan model is easier to map to variable pairs: Wide format with different rows for outcomes (well-being) and predictors (hours, needs, motivations).

```{r}
d_riclpm_long <- d %>%
  ### !! Remove this when data is available !!
  filter(!(Game %in% c("Forza Horizon 4", "Gran Turismo Sport"))) %>%
  mutate(Hours = coalesce(Hours, hours_est)) %>%
  # Make coefficients scales more reasonable by dividing hours by ten
  mutate(Hours = Hours / 10) %>%
  select(Game, pid, wid, Hours, Needs, Enjoyment, SPANE, CSAS) %>%
  # Long format on well-being scores (outcomes)
  pivot_longer(
    c(SPANE, CSAS), names_to = "y_var", values_to = "y"
  ) %>%
  # Long format on other variables (predictors)
  pivot_longer(
    c(Hours, Needs, Enjoyment), 
    names_to = "x_var", values_to = "x"
  )
d_riclpm_wide <- d_riclpm_long %>% 
  pivot_wider(names_from = wid, values_from = c(x, y), names_sep = "")
```

## Some kind of data figure

```{r}
Order <- tibble(
  y_var = c("SPANE", "CSAS", "SPANE", "CSAS"),
  wid = c(2, 2, 3, 3),
  Panel = c(
    "Affect[plain('[Wave 2]')]",
    "Life~satisfaction[plain('[Wave 2]')]",
    "Affect[plain('[Wave 3]')]",
    "Life~satisfaction[plain('[Wave 3]')]"
  ),
  Panel_label = fct_inorder(Panel)
)

d_riclpm_plots <- d_riclpm_long %>% 
  arrange(Game, pid, x_var, wid) %>%
  group_by(Game, pid, x_var, y_var) %>%
  mutate(lag_x = lag(x)) %>%
  left_join(Order) %>% 
  ungroup() %>% 
  filter(wid > 1)

d_riclpm_plots %>% 
  filter(x_var == "Hours", lag_x <= 5) %>% 
  ggplot(aes(lag_x, y, col = Game)) + 
  scale_x_continuous(
    "Hours played at previous wave",
    breaks = pretty_breaks(), 
    labels = function(x) x*10
  ) +
  scale_y_continuous(
    "Well-being at current wave",
    breaks = pretty_breaks()
  ) +
  scale_color_brewer(palette = "Set1", aesthetics = c("color", "fill")) +
  geom_smooth(
    method = "lm", size = .4,
    alpha = .2, show.legend = FALSE
  ) +
  geom_point(size = .2, alpha = .5) +
  guides(
    color = guide_legend(
      override.aes = list(size = 2, shape = 16, alpha = 1),
      nrow = 2, byrow = TRUE
    ),
    fill = guide_legend(nrow = 2, byrow = TRUE)
  ) +
  theme(
    aspect.ratio = 1,
    legend.position = "bottom", 
    legend.justification = "left", 
    legend.margin = margin(l = -12),
    legend.title = element_blank(), 
    legend.text = element_text(margin = margin(l = -8), size = 8)
  ) +
  facet_wrap("Panel_label", scales = "free_y", labeller = label_parsed)
ggsave(here("Output/Figures/Figure-2.png"), width = 5, height = 4)

# How many x values are over 50?
last_plot()$data %>% 
  mutate(too_great = lag_x >= 5) %>%
  tabyl(too_great) %>%
  adorn_pct_formatting()
```

## Fit one model to all data

First fit RICLPM to all data. Separately to the two well-being outcomes, and each predictor (hours, subjective scales). Constrain (cross)lagged parameters

Syntax based on <https://jeroendmulder.github.io/RI-CLPM/lavaan.html>

```{r}
riclpm_constrained <- "
  # Create between components (random intercepts)
  RIx =~ 1*x1 + 1*x2 + 1*x3
  RIy =~ 1*y1 + 1*y2 + 1*y3

  # Create within-person centered variables
  wx1 =~ 1*x1
  wx2 =~ 1*x2
  wx3 =~ 1*x3
  wy1 =~ 1*y1
  wy2 =~ 1*y2
  wy3 =~ 1*y3

  # Estimate the lagged effects between the within-person centered variables (constrained).
  wx2 ~ bx*wx1 + gx*wy1
  wy2 ~ gy*wx1 + by*wy1
  wx3 ~ bx*wx2 + gx*wy2
  wy3 ~ gy*wx2 + bx*wy2

  # Estimate the covariance between the within-person centered
  # variables at the first wave.
  wx1 ~~ wy1 # Covariance

  # Estimate the covariances between the residuals of the
  # within-person centered variables (the innovations).
  wx2 ~~ wy2
  wx3 ~~ wy3

  # Estimate the variance and covariance of the random intercepts.
  RIx ~~ RIx
  RIy ~~ RIy
  RIx ~~ RIy

  # Estimate the (residual) variance of the within-person centered variables.
  wx1 ~~ wx1 # Variances
  wy1 ~~ wy1
  wx2 ~~ wx2 # Residual variances
  wy2 ~~ wy2
  wx3 ~~ wx3
  wy3 ~~ wy3
"
```

```{r}
fit_riclpm_all <- d_riclpm_wide %>%
  group_by(y_var, x_var) %>%
  summarise(
    fit = lavaan(
      riclpm_constrained,
      data = cur_data(),
      missing = "ml",
      meanstructure = TRUE,
      int.ov.free = TRUE
    ) %>% list()
  )

get_lavaan_pars <- function(x) {
  bind_rows(
    parameterestimates(x) %>%
      mutate(Type = "Unstandardized"),
    standardizedsolution(x) %>%
      rename(est = est.std) %>%
      mutate(Type = "Standardized")
  ) %>%
    as_tibble() %>%
    unite("Parameter", c(lhs, op, rhs), sep = " ", remove = FALSE)
}

pars_riclpm_avg <- fit_riclpm_all %>%
  mutate(pars = map(fit, get_lavaan_pars)) %>%
  select(-fit)
```

## Separate models per game

Fit the above model separately to each game.

```{r}
fit_riclpm_sep <- d_riclpm_wide %>%
  group_by(y_var, x_var, Game) %>%
  summarise(
    fit = lavaan(
      riclpm_constrained,
      data = cur_data(),
      missing = "ml",
      meanstructure = TRUE,
      int.ov.free = TRUE
    ) %>% list()
  )
pars_riclpm_sep <- fit_riclpm_sep %>%
  mutate(pars = map(fit, get_lavaan_pars)) %>%
  select(-fit)
```

## Multi-group fit with parameter constraints

Fit the model to all the data but allow unique (cross)lagged parameters for each game. This may help if some data sets are too small to fit the model completely independently.

```{r}
riclpm_constrained_multigroup <- "
  # Create between components (random intercepts)
  RIx =~ 1*x1 + 1*x2 + 1*x3
  RIy =~ 1*y1 + 1*y2 + 1*y3

  # Create within-person centered variables
  wx1 =~ 1*x1
  wx2 =~ 1*x2
  wx3 =~ 1*x3
  wy1 =~ 1*y1
  wy2 =~ 1*y2
  wy3 =~ 1*y3

  # Estimate the lagged effects between the within-person centered variables (constrained)
  # such that each group gets their own coefs
  wx2 ~ c(bx1, bx2, bx3, bx4, bx5)*wx1 + c(gx1, gx2, gx3, gx4, gx5)*wy1
  wy2 ~ c(gy1, gy2, gy3, gy4, gy5)*wx1 + c(by1, by2, by3, by4, by5)*wy1
  wx3 ~ c(bx1, bx2, bx3, bx4, bx5)*wx2 + c(gx1, gx2, gx3, gx4, gx5)*wy2
  wy3 ~ c(gy1, gy2, gy3, gy4, gy5)*wx2 + c(by1, by2, by3, by4, by5)*wy2

  # Estimate the covariance between the within-person centered
  # variables at the first wave.
  wx1 ~~ wy1 # Covariance

  # Estimate the covariances between the residuals of the
  # within-person centered variables (the innovations).
  wx2 ~~ wy2
  wx3 ~~ wy3

  # Estimate the variance and covariance of the random intercepts.
  RIx ~~ RIx
  RIy ~~ RIy
  RIx ~~ RIy

  # Estimate the (residual) variance of the within-person centered variables.
  wx1 ~~ wx1 # Variances
  wy1 ~~ wy1
  wx2 ~~ wx2 # Residual variances
  wy2 ~~ wy2
  wx3 ~~ wx3
  wy3 ~~ wy3
"
```

```{r}
fit_riclpm_mg <- d_riclpm_wide %>%
  group_by(y_var, x_var) %>%
  summarise(
    fit = lavaan(
      riclpm_constrained_multigroup,
      data = cur_data(),
      missing = "ml",
      group = "Game",
      meanstructure = TRUE,
      int.ov.free = TRUE
    ) %>% list()
  )

pars_riclpm_mg <- fit_riclpm_mg %>%
  mutate(pars = map(fit, get_lavaan_pars)) %>%
  mutate(
    pars = map2(
      pars, fit,
      ~ left_join(
        .x, 
        tibble(
          group = 1:length(unique(d_riclpm$Game)), 
          Game = .y@Data@group.label
        )
      )
    )
  ) %>%
  select(-fit)
```

Look at estimates

```{r}
# Table of RICLPM parameters
pars_riclpm <- pars_riclpm_mg %>%
  unnest(pars) %>%
  group_by(y_var, x_var, Game) %>%
  nest() %>%
  rename(pars = data) %>%
  mutate(Model = "Multigroup") %>%
  bind_rows(pars_riclpm_sep %>% mutate(Model = "Independent")) %>%
  bind_rows(pars_riclpm_avg %>% mutate(Model = "Average", Game = "Average")) %>%
  ungroup() %>%
  unnest(pars) %>%
  # Take regression parameters at first interval, and 
  # covariance parameters of interest
  filter(
    (str_detect(Parameter, " ~ ") & !str_detect(Parameter, "3")) |
    Parameter %in% c("RIx ~~ RIy", "wx1 ~~ wy1", "wx2 ~~ wy2", "wx3 ~~ wy3")
  ) %>%
  mutate(Game = fct_relevel(Game, "Average"))

# Plot of regression parameters (autocorrelations and crosslags; raw)
pars_riclpm %>%
  rename(Predictor = x_var, Outcome = y_var) %>% 
  filter(Type == "Unstandardized", str_detect(Parameter, " ~ ")) %>% 
  ggplot(
    aes(
      est, Game, 
      col = Model, xmin = ci.lower, xmax = ci.upper, shape = Outcome
    )
  ) +
  scale_x_continuous(
    "Estimate",
    breaks = pretty_breaks()
  ) +
  geom_vline(xintercept = 0, size = .2, lty = 2) +
  geom_pointrangeh(position = position_dodge2v(.5), size = .4) +
  facet_grid(Parameter ~ Predictor, scales = "free_x", labeller = label_both)

# Plots of covariance parameters (raw)
last_plot() %+% 
  filter(
    pars_riclpm, 
    Type == "Unstandardized", 
    str_detect(Parameter, " ~~ ")
  )
```

We need to decide whether to use multigroup estimates or independent model estimates. Potentially former and maybe give more game-specific parameters in the model. For now we have gone with independent model estimates.

# Meta-analyses

Number of observations/participants per model

```{r}
fit_riclpm_sep %>%
  mutate(N = map_dbl(fit, nobs) %>% comma) %>% 
  select(-fit) %>% 
  pivot_wider(
    names_from = x_var, 
    values_from = N, 
    names_glue = "{x_var} {.value}"
  ) %>% 
  rename(Outcome = y_var) %>% 
  kable(caption = "Sample sizes per RICLPM")
```

Then conduct all meta-analyses (for each y_var-x_var pair, parameter of interest, and type of parameter (standardized, unstandardized)) based on the independent fit RICLPMs

```{r results='hide'}
# Meta-analyze the models fitted independently to games
# Select these parameters only
d_ma <- pars_riclpm %>%
  filter(Model == "Independent", str_detect(Parameter, " ~ "))

# Compile meta-analytic brms/Stan model
bf_ma <- bf(est | se(se) ~ 0 + Intercept + (0 + Intercept | Game))
fit_ma_empty <- brm(
  bf_ma,
  data = d_ma,
  prior = prior(student_t(7, 0, 0.25), class = "sd", group = "Game") +
    prior(normal(0, 0.5), class = "b"),
  chains = 0,
  control = list(adapt_delta = .9999, max_treedepth = 15),
  file = here("Output/brms/brm-ma-empty")
)

# Fit meta-analysis separately to each x-y pair X parameter X type 
# by updating the compiled model (48 meta-analyses)
fit_ma <- d_ma %>%
  group_by(x_var, y_var, Parameter, lhs, rhs, Type) %>%
  summarise(
    fit = list(
      update(
        fit_ma_empty,
        newdata = cur_data(),
        control = list(adapt_delta = .9999, max_treedepth = 15),
        iter = 15000, warmup = 5000, 
        refresh = 0,
        file = here(str_glue("Output/brms/brm-ma-{cur_group_id()}"))
      )
    )
  )

# Function to get varying and average effects' posteriors from brmsfit
get_ma_post <- function(x) {
  coef(x, summary = FALSE) %>%
    .[["Game"]] %>%
    .[, , 1] %>%
    as.data.frame() %>%
    as_tibble() %>%
    cbind(fixef(x, summary = FALSE))
}
```

## Hours <-> WB

Draw a forest plot of meta-analyses with hours

```{r}
# Create nicer labels for variables and panels
Order <- distinct(fit_ma, x_var, y_var, Parameter, lhs, rhs) %>% 
  ungroup() %>% 
  # Meta-analyses only for regression parameters
  filter(str_detect(Parameter, " ~ ")) %>% 
  mutate(
    x_lab = case_when(
      x_var == "Hours" ~ "Play",
      x_var == "Enjoyment" ~ "Intrinsic~motivation",
      x_var == "Needs" ~ "Need~satisfaction"
    ),
    y_lab = case_when(
      y_var == "SPANE" ~ "Affect",
      y_var == "CSAS" ~ "Life~satisfaction"
    ),
    Panel_label = case_when(
      Parameter == "wx2 ~ wx1" ~ 
        str_glue('{x_lab}[plain("[t-1]")]%->%{x_lab}[plain("[t]")]'),
      Parameter == "wy2 ~ wy1" ~ 
        str_glue('{y_lab}[plain("[t-1]")]%->%{y_lab}[plain("[t]")]'),
      Parameter == "wy2 ~ wx1" ~ 
        str_glue('{x_lab}[plain("[t-1]")]%->%{y_lab}[plain("[t]")]'),
      Parameter == "wx2 ~ wy1" ~ 
        str_glue('{y_lab}[plain("[t-1]")]%->%{x_lab}[plain("[t]")]')
    )
  ) %>% 
  mutate(
    Parameter_order = factor(
      Parameter, 
      levels = c("wy2 ~ wx1", "wx2 ~ wy1", "wy2 ~ wy1", "wx2 ~ wx1")
      )
    ) %>% 
  arrange(Parameter_order, desc(y_var)) %>% 
  mutate(Panel_label = fct_inorder(Panel_label))
# Did ordering work?
# Order %>% 
#   filter(x_var == "Enjoyment", Parameter %in% c("wy2 ~ wx1", "wx2 ~ wy1")) %>% 
#   mutate(n = as.numeric(Panel_label))

# Table of ROPEs
# Smallest subjective change as a percentage from Anvari&Lakens
sscp <- .2 / 5
rope_limits <- distinct(fit_ma, y_var, Parameter, Type) %>%
  mutate(
    rope_limit = case_when(
      Type == "Standardized" ~ .1,
      Type == "Unstandardized" & Parameter == "wx2 ~ wy1" ~ .1,
      y_var == "CSAS" ~ (sscp * 11) / 3,
      y_var == "SPANE" ~ (sscp * 13) / 3
    )
  )

# Data for plotting meta-analyses' posterior distributions
p_data <- fit_ma %>%
  mutate(out = map(fit, get_ma_post)) %>%
  select(-fit) %>%
  unnest(out) %>%
  rename(Average = Intercept) %>%
  pivot_longer(`AC:NH`:Average, names_to = "Game") %>%
  mutate(Game = fct_relevel(Game, "Average")) %>%
  ungroup() %>%
  left_join(Order) %>% 
  left_join(rope_limits)

# Summaries of posterior distributions
fit_ma_sum <- p_data %>%
  left_join(rope_limits) %>%
  group_by(y_var, x_var, Parameter, Type, Game, Panel_label, rope_limit) %>%
  summarise(
    describe_posterior(
      value,
      rope_range = c(-unique(rope_limit), unique(rope_limit)),
      test = c("ps", "rope"), rope_ci = 1,
      centrality = "mean"
    ) %>% select(-Parameter)
  ) %>%
  ungroup() %>%
  mutate(
    across(
      c(Mean, CI_low, CI_high),
      .fns = list(r = ~ format(round(., 2), nsmall = 2))
    )
  ) %>%
  mutate(Res = str_glue("{Mean_r} [{CI_low_r}, {CI_high_r}]")) %>%
  # Probability outside ROPE in other direction
  mutate(p_other = 1 - (ps + ROPE_Percentage)) %>%
  # Ordered labels
  left_join(Order) %>%
  mutate(Game = fct_relevel(Game, "Average"))

# Function to draw forest plots
forest_plot <- function(
  type = "Unstandardized", 
  x_var = "Hours", 
  parameters = c("wy2 ~ wx1", "wx2 ~ wy1"),
  lavaan = FALSE
  ) {
  out <- fit_ma_sum %>%
    filter(
      Type == {{type}}, 
      x_var == {{x_var}}, 
      Parameter %in% {{parameters}}
    ) %>%
    # We can use the xintercept mapping to control alpha below
    ggplot(aes(Mean, Game, xintercept = rope_limit)) +
    coord_cartesian(xlim = c(-.4, .55)) +
    scale_x_continuous(
      "Estimated cross-lagged effect",
      breaks = c(-.3, -.2, -.1, 0, .1, .2, .3, .4),
      expand = expansion(.01)
    ) +
    scale_y_discrete(
      expand = expansion(c(0.15, 0.25)),
      labels = function(x) ifelse(x == "Average", "**Average**", x)
    ) +
    scale_alpha_manual(values = c(.33, 1)) +
    scale_fill_brewer(
      palette = "Set1", direction = 1, aesthetics = c("fill", "color")
    ) +
    # Vertical lines for ROPE and 0
    geom_vline(xintercept = 0, size = .1, col = "grey60") +
    geom_vline(
      aes(xintercept = rope_limit),
      size = .1, lty = 2, col = "grey60"
    ) +
    geom_vline(
      aes(xintercept = -rope_limit),
      size = .1, lty = 2, col = "grey60"
    ) +
    # Posterior densities
    stat_halfeye(
      data = p_data %>% 
        filter(
          Type == {{type}}, 
          x_var == {{x_var}}, 
          Parameter %in% {{parameters}}
        ),
      aes(
        value,
        fill = stat(x > 0),
        alpha = after_stat(abs(x) > xintercept)
      ),
      height = .5, adjust = 1.5, point_interval = NULL,
      show.legend = FALSE, normalize = "panels"
    ) +
    # Summary geoms and texts
    geom_pointrangeh(
      aes(x = Mean, xmin = CI_low, xmax = CI_high),
      size = .33, fatten = 1.5
    ) +
    # CIs in right margin
    geom_text(
      vjust = -0.5, size = 3, hjust = 1,
      aes(x = .55, label = Res),
      family = Font
    ) +
    # Posterior probabilities for average
    geom_text(
      data = fit_ma_sum %>%
        filter(Game == "Average") %>%
        filter(
          Type == {{type}}, 
          x_var == {{x_var}}, 
          Parameter %in% {{parameters}}
        ),
      vjust = 1.4, size = 3,
      aes(
        x = sign(Mean) * rope_limit,
        hjust = ifelse(sign(Mean) == 1, 0, 1),
        label = str_glue("{percent(ps, .1)}"),
        col = as.logical(sign(Mean) == 1)
      ),
      family = Font,
      show.legend = FALSE
    ) +
    geom_text(
      data = fit_ma_sum %>%
        filter(Game == "Average") %>%
        filter(
          Type == {{type}}, 
          x_var == {{x_var}}, 
          Parameter %in% {{parameters}}
        ),
      vjust = 1.4, size = 3,
      aes(
        x = 0,
        hjust = 0.5,
        label = str_glue("{percent(ROPE_Percentage, .1)}")
      ),
      col = "gray50",
      family = Font,
      show.legend = FALSE
    ) +
    geom_text(
      data = fit_ma_sum %>%
        filter(Game == "Average") %>%
        filter(
          Type == {{type}}, 
          x_var == {{x_var}}, 
          Parameter %in% {{parameters}}
        ),
      vjust = 1.4, size = 3,
      aes(
        x = -sign(Mean) * rope_limit,
        hjust = ifelse(sign(Mean) == 1, 1, 0),
        label = str_glue("{percent(p_other, .1)}"),
        col = as.logical(-sign(Mean) == 1)
      ),
      family = Font,
      show.legend = FALSE
    ) +
    theme(
      axis.title.y = element_blank(),
      axis.text.y = element_markdown(),
      # panel.grid.major.y = element_blank()
    ) +
    facet_wrap("Panel_label", scales = "free_x", labeller = label_parsed)
  
  # Also display lavaan game-specific estimates?
  if (lavaan) {
    out +
      geom_pointrangeh(
        data = d_ma %>%
          left_join(Order) %>% 
          filter(
            Type == {{type}}, 
            x_var == {{x_var}}, 
            Parameter %in% {{parameters}}
          ) %>% 
          mutate(rope_limit=.1),
        col = "gray40", fatten = 1.25, size = .33,
        aes(x = est, xmin = ci.lower, xmax = ci.upper),
        position = position_nudge(y = -.075)
      )
  } else {out}
}
forest_plot(
  "Unstandardized", 
  "Hours",
  parameters = c("wy2 ~ wx1", "wx2 ~ wy1"), 
  lavaan = FALSE
)
ggsave(here("Output/Figures/Figure-3-Unstandardized.png"), width = 8, height = 5)
# forest_plot("Standardized")
# ggsave(here("Output/Figures/Figure-3-Standardized.png"), width = 8, height = 5)
```

```{r}
# Also see in comparison to individual model estimates
forest_plot(
  "Unstandardized", 
  "Hours",
  parameters = c("wy2 ~ wx1", "wx2 ~ wy1"), 
  lavaan = TRUE
)
```

Table?

```{r}
fit_ma_sum %>%
  select(y_var, x_var, Game, Parameter, Type, Res) %>% 
  DT::datatable(filter = "top")
```


## Experiences <-> WB

These meta-analyses were conducted above. Now we just need to filter for the correct parameters for plots. They are also shown in the table above. Need to double check that everything has worked so these meta analyses are still a bit of WIP.

```{r}
forest_plot(
  "Unstandardized", 
  "Enjoyment",
  parameters = c("wy2 ~ wx1"), 
  lavaan = FALSE
) + theme(axis.title.x = element_blank()) -
forest_plot(
  "Unstandardized", 
  "Needs",
  parameters = c("wy2 ~ wx1"), 
  lavaan = FALSE
) +
  plot_layout(nrow = 2)
ggsave(
  here("Output/Figures/Figure-ma-subjective-Unstandardized.png"), 
  width = 8, 
  height = 5
  )
```

